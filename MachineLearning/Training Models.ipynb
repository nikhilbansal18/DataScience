{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "billion-seating",
   "metadata": {},
   "source": [
    "## Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conservative-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Normal Equation\n",
    "# To find the value of θ that minimizes the cost function, there is a closed-form solution —in other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "significant-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruled-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s compute θ using the Normal Equation\n",
    "X_b = np.c_[np.ones((100, 1)), X] \n",
    "# add x0 = 1 to each instance theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grateful-phrase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.77569315],\n",
       "       [2.98643946]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "touched-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.77569315]), array([[2.98643946]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "lin_reg.intercept_,lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lasting-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-italian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.77569315],\n",
       "       [9.74857207]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-amber",
   "metadata": {},
   "source": [
    "#### Now we will look at very different ways to train a Linear Regression model, better suited for cases where there are a large number of features, or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-sierra",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n",
    "\n",
    "When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s StandardScaler class), or else it will take much longer to converge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-psychology",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "\n",
    "To implement Gradient Descent, you need to compute the gradient of the cost func‐ tion with regards to each model parameter θj. In other words, you need to calculate how much the cost function will change if you change θj just a little bit. This is called a partial derivative.\n",
    "Notice that this formula involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alert-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 #learning rate\n",
    "## To find a good learning rate, you can use grid search \n",
    "n_iterations = 1000\n",
    "## A simple solu‐ tion is to set a very large number of iterations but to interrupt the algorithm when the gradient vector \n",
    "## becomes tiny—that is, when its norm becomes smaller than a tiny number ε (called the tolerance)—\n",
    "## because this happens when Gradient Descent has (almost) reached the minimum.\n",
    "m=100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initalization\n",
    "\n",
    "for iterations in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cellular-joyce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.77569315],\n",
       "       [2.98643946]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta\n",
    "\n",
    "# that’s exactly what the Normal Equation found! Gradient Descent worked per‐ fectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-briefing",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. \n",
    "\n",
    "At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance.\n",
    "\n",
    "Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (SGD can be implemented as an out-of-core algorithm.7)\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on aver‐ age.\n",
    "\n",
    "Therefore randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is akin to simulated anneal‐ ing\n",
    "\n",
    "The function that determines the learning rate at each iteration is called the learning schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "genuine-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t): \n",
    "    return t0/(t+t1)\n",
    "\n",
    "    theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs): \n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indie-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.73427942],\n",
       "       [2.93500816]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attached-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe gressor class\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol= 1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X,y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "determined-option",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.79882926]), array([3.00775606]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_,sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-closer",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent\n",
    "\n",
    "at each step, instead of computing the gradients based on the full train‐ ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini- batch GD computes the gradients on small random sets of instances called mini- batches. \n",
    "\n",
    "The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.\n",
    "\n",
    "shows the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐ tic GD and Mini-batch GD would also reach the minimum if you used a good learn‐ ing schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-topic",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "What if your data is actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to fit nonlinear data. \n",
    "\n",
    "A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ranking-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "steady-terminology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "greenhouse-generation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.85969896])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Clearly, a straight line will never fit this data properly.\n",
    "## So let’s use Scikit-Learn’s Poly nomialFeatures class\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "inappropriate-benchmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.85969896,  0.7390823 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "naughty-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.947836]), array([[1.01029034, 0.54018983]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-garage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
